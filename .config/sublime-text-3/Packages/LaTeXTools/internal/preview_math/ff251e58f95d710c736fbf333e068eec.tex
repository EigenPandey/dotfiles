
\documentclass[preview,border=0.3pt]{standalone}
% import xcolor if available and not already present
\IfFileExists{xcolor.sty}{\usepackage{xcolor}}{}%
\usepackage{amsmath}
\usepackage{amssymb}
\IfFileExists{latexsym.sty}{\usepackage{latexsym}}{}
\IfFileExists{mathtools.sty}{\usepackage{mathtools}}{}

\begin{document}
% set the foreground color
\IfFileExists{xcolor.sty}{\color[HTML]{EEFFFF}}{}%
$$
\begin{figure}[h]
    \centering
    \includestandalone[width=0.6\textwidth]{train_rkm_schematic}
    \caption[]{Gen-RKM schematic representation modeling a common subspace $\mathcal{H}$ between two data sources $\mathcal{X}$ and $\mathcal{Y}$. The $\phi_1$, $\phi_2$ are the feature maps ($\mathcal{F}_x$ and $\mathcal{F}_y$ represent the feature-spaces) corresponding to the two data sources. While $\psi_1$, $\psi_2$ represent the pre-image maps. The interconnection matrices $\bm{U},\bm{V}$ model dependencies between latent variables and the mapped data sources.}
    \label{fig:schematic_Gen_rkm}
\end{figure}
%
Given $ \eta_{1} > 0 $ and $ \eta_{2} > 0 $ as regularization parameters, the stationary points of $\mathcal{J}_{t}$ are given by:
%
\begin{equation}\label{eq:2}
    \begin{cases}
        \frac{\partial \mathcal{J}_{t}}{\partial \bm{h}_{i}}=0  \implies & {\lambda \bm{h}_i =  \bm{U}^\top \phi_{1}(\bm{x}_i) +\bm{V}^\top \phi_{2}(\bm{y}_i)}, \enskip \forall i= 1,\dots, N \\
        \frac{\partial \mathcal{J}_{t}}{\partial \bm{U}}=0     \implies  & {\bm{U} = \frac{1}{\eta_{1}} \sum_{i=1}^{N}\phi_{1}(\bm{x}_{i})\bm{h}_{i}^\top }                                    \\
        \frac{\partial \mathcal{J}_{t}}{\partial \bm{V}} =0     \implies & {\bm{V} = \frac{1}{\eta_{2}} \sum_{i=1}^{N}\phi_{2}(\bm{y}_{i})\bm{h}_{i}^\top }.
    \end{cases}
\end{equation}
%
Substituting $\bm{U}$ and $\bm{V}$ in the first equation above, denoting $ \bm{\Lambda} =\diag\{\lambda_1,\ldots,\lambda_s\}\in\mathbb{R}^{s\times s} $ with $s \leq N$, yields the following eigenvalue problem:
%
\begin{equation}\label{eq:sup_KPCA}
    {\left[ \frac{1}{\eta_{1}}\bm{K}_{1} + \frac{1}{\eta_{2}}\bm{K}_{2} \right] \bm{H}^\top=\bm{H}^\top \bm{\Lambda}},
\end{equation}
%
where $ \bm{H} =\big[\bm{h}_1 ,\dots, \bm{h}_N \big]\in \mathbb{R}^{s\times N} $ with $s\leq N$ is the number of selected principal components and $\bm{K}_{1},\bm{K}_{2} \in \mathbb{R}^{N \times N}$ are the kernel matrices corresponding to data sources\footnote{While in the above section we have assumed that only two data sources (namely $\mathcal{X}$ and $\mathcal{Y}$) are available for learning, the above procedure could be extended to multiple data-sources. For the $M$ views or data-sources, this yields the training problem: ${\left[\sum_{\ell=1}^{M} \frac{1}{\eta_{\ell}}\bm{K}_{\ell} \right] \bm{H}^\top=\bm{H}^\top \bm{\Lambda}}.$}. Based on Mercer's theorem \cite{mercer_james_functions}, positive-definite kernel functions $k_{1}: \mathbb{R}^{d} \times \mathbb{R}^{d} \mapsto \mathbb{R}$, $k_{2}: \mathbb{R}^{p} \times \mathbb{R}^{p} \mapsto \mathbb{R}$  can be defined such that $k_{1}(\bm{x}_{i},\bm{x}_{j})=\langle \phi_{1}(\bm{x}_{i}), \phi_{1}(\bm{x}_{j})\rangle$, and $k_{2}(\bm{y}_{i},\bm{y}_{j})=\langle \phi_{2}(\bm{y}_{i}), \phi_{2}(\bm{y}_{j})\rangle,~\forall i,j=1,\dots,N$ forms the elements of corresponding kernel matrices. The feature maps $\phi_{1}$ and $\phi_{2}$, mapping the input data to the high-dimensional feature space (possibly infinite) are implicitly defined by kernel functions. Typical examples of such kernels are given by the Gaussian RBF kernel $k(\bm{x}_i,\bm{x}_j) = e^{-\|\bm{x}_i-\bm{x}_j\|_2^2/(2\sigma^2)}$ or the Laplace kernel $k(\bm{x}_i,\bm{x}_j) =e^{-\|\bm{x}_i-\bm{x}_j\|_2/\sigma}$ just to name a few \cite{Scholkopf2001}. However, one can also define explicit feature maps, still preserving the positive-definiteness of the kernel function by construction \cite{suykens_least_2002}.\\
% However, we will mostly use $ H $ to work with non-linear models. From training, we know $ V $, $ U $ and $ H $.
%
%
\subsection{Generation}
%
In this section, we derive the equations for the generative mechanism. RKMs resembling energy-based models, the inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimizes the energy  \cite{lecun_learning_2004}. Given the learned interconnection matrices $\bm U$ and $\bm V$, and a given latent variable $\bm{h}^{\star}$, consider the following objective function:
%
\begin{equation}\label{eq:obj_gen_lin}
    {\mathcal{J}_{g}= - \phi_{1}(\bm{x}^{\star})^{\top}\bm{U}\bm{h}^{\star} - \phi_{2}(\bm{y}^{\star})^{\top}\bm{V} \bm{h}^{*} + \dfrac{1}{2}\phi_{1}(\bm{x}^{\star})^{\top}\phi_{1}(\bm{x}^{\star}) + \dfrac{1}{2}\phi_{2}(\bm{y}^{\star})^{\top}\phi_{2}(\bm{y}^{\star})},
\end{equation}
with an additional regularization term on data sources. The given latent variable $\bm{h}^{\star}$ can be the corresponding hidden variable of a training point, a newly sampled hidden unit or a specifically determined one. Above cases correspond to generating the reconstructed visible unit, generating a random new visible unit or exploring the latent space by carefully selecting hidden units respectively.
Here ${\mathcal{J}_{g}}$ denotes the objective function for generation.
%
The stationary points of $\mathcal{J}_{g}$ are characterized by:
%
\begin{equation}\label{eq:statio_pred}
    \begin{cases}
        \frac{\partial \mathcal{J}_{g}}{\partial \phi_{1}(\bm{x}^{\star})}=0     \implies  & \phi_{1}(\bm{x}^{\star}) = \bm{U}\bm{h}^{\star}, \\
        \frac{\partial \mathcal{J}_{g}}{\partial \phi_{2}(\bm{y}^{\star})} =0     \implies & \phi_{2}(\bm{y}^{\star}) = \bm{V}\bm{h}^{\star}.
    \end{cases}
\end{equation}
%
Using $\bm{U}$ and $\bm V$ from Eq. \ref{eq:2}, we obtain the generated feature vectors:
%
\begin{equation}\label{eq:impl_sol}
    {\phi}_{1}(\bm{x}^{\star}) = \left(\dfrac{1}{\eta_{1}} \sum_{i=1}^{N}{\phi}_{1}(\bm{x}_i)\bm{h}_{i}^\top\right) \bm{h}^{\star},
    \quad
    {\phi}_{2}(\bm{y}^{\star}) = \left(\dfrac{1}{\eta_{2}} \sum_{i=1}^{N}{\phi}_{2}(\bm{y}_i)\bm{h}_{i}^\top\right) \bm{h}^{\star}.
\end{equation}
%
%
%
%
To obtain the generated data, one now needs to compute the inverse images of the feature maps $\phi_{1}(\cdot)$ and $\phi_{2}(\cdot)$ in the respective input spaces, i.e., solve the \emph{pre-image problem}. We seek to find the functions $\psi_1 \colon \mathbb{R}^{d_f}  \mapsto \mathbb{R}^{d}$ and $\psi_2 \colon \mathbb{R}^{p_f}  \mapsto \mathbb{R}^p$ corresponding to the two data-sources, such that $(\psi_1 \circ \phi_1) (\bm{x}^\star)\approx \bm{x}^\star$ and $(\psi_2 \circ \phi_2) (\bm{y}^\star) \approx \bm{y}^\star$, where $\phi_1 (\bm{x}^\star)$ and $\phi_2 (\bm{y}^\star)$ are calculated using Eq. \ref{eq:impl_sol}.

When using kernel methods, explicit feature maps are not necessarily known. Commonly used kernels such as the radial-basis function and polynomial kernels map the input data to a very high dimensional feature space. Hence finding the pre-image, in general, is known to be an ill-conditioned problem \cite{mika_kernel_nodate}. However, various approximation techniques have been proposed \cite{bui_projection-free_2019,kwok_pre-image_2004-2,honeine_preimage_2011-1,weston_learning_2004} which could be used to obtain the approximate pre-image $\hat{\bm{x}}$ of ${\phi}_{1}(\bm{x}^{\star})$. In section \ref{sec:implicit}, we employ one such technique to demonstrate the applicability in our model, and consequently generate the multi-view data. One could also define explicit pre-image maps. In section \ref{sec: explicit}, we define parametric pre-image maps and learn the parameters by minimizing the appropriately defined reconstruction errors. The next section describes the above two pre-image methods for both cases, i.e., when the feature map is explicitly known or unknown, in greater detail.
%
% \subsubsection{Estimating Pre-images}
% % (new pre-image, neural network)
% Here we outline two methods for estimating pre-image both when feature map $\phi$ is known explicitly known or not. Since $ \bm{x}^{\star} $ is not unique or may not exist, we find an approximation $ \hat{\bm{x}} $:
% 
\section{Implicit \& Explicit feature map \label{sec:featuremaps}}
\subsection{Implicit feature map}
\label{sec:implicit}
%
As noted in the previous section, since $ \bm{x}^{\star} $ may not exist, we find an approximation $ \hat{\bm{x}} $. A possible technique is shown by cite{joachim}. Left multiplying Eq. \ref{eq:impl_sol} by ${\phi}_{1}(\bm{x}_{i}^{\star})^{\top}$ and ${\phi}_{2}(\bm{y}_{i}^{\star})^{\top}$, $\forall i=1,\dots, N$, we obtain:
% 
\begin{equation}\label{eq:Kxy}
    {{\bm k}_{\bm{x}^{\star}}=\dfrac{1}{\eta_{1}}{\bm{K}}_{1}\bm{H}^\top \bm{h}^{\star}}, \quad    {{\bm{k}}_{\bm{y}^{\star}}=\dfrac{1}{\eta_{2}}{\bm{K}}_{2}\bm{H}^\top \bm{h}^{\star}},
\end{equation}
where, ${\bm k}_{\bm{x}^{\star}}=\left[ k(\bm{x}_{1}, \bm{x}^{\star}),\dots,k(\bm{x}_{N}, \bm{x}^{\star}) \right]^{\top}$ represents the \emph{similarities} between $\phi_1 (\bm{x}^{\star})$ and training data points in the feature space, and $\bm{K}_{1}\in \mathbb{R}^{N\times N}$ represents the centered kernel matrix of $\mathcal{X}$. Similar conventions follow for $\mathcal{Y}$ respectively. Using the \emph{kernel-smoother} method \cite{hastie01statisticallearning}, the pre-images are given by:
%
\begin{equation}\label{eq: pre-Ix,y}
    {\hat{\bm{x}} = \psi_1 \left( \phi_1 (\bm{x}^\star) \right) =\dfrac{\sum_{j=1}^{n_{r}} \tilde{k}_{1}(\bm{x}_{j}, \bm{x}^{\star})\bm{x}_j }{\sum_{j=1}^{n_{r}} \tilde{k}_{1}(\bm{x}_{j}, \bm{x}^{\star})}}, \quad    {\hat{\bm{y}}= \psi_2 \left( \phi_2 (\bm{y}^\star) \right) = \dfrac{\sum_{j=1}^{n_{r}} \tilde{k}_{2}(\bm{y}_{j}, \bm{y}^{\star})\bm{y}_j }{\sum_{j=1}^{n_{r}} \tilde{k}_{2}(\bm{y}_{j}, \bm{y}^{\star})}},
\end{equation}
where $\tilde{k}_1(\bm{x}_i,\bm{x}^{\star})$ and $\tilde{k}_2(\bm{y}_i,\bm{y}^{\star})$ are the scaled similarities (see Eq. \ref{eq: pre-Ix,y}) between $0$ and $1$ and $n_{r}$ the number of closest points based on the similarity defined by kernels $\tilde{k}_1$ and $\tilde{k}_2$.
% Advantages: Targeted generation (useful in critical applications)
% 
\subsection{Explicit Feature map}
\label{sec: explicit}

While using an explicit feature map, Mercer's theorem still holds due to the positive  semi-definiteness of the kernel function by construction, thereby allowing the derivation of Eq. \ref{eq:sup_KPCA}. In the experiments, we use a set of (convolutional) neural networks as the feature maps $\phi_{\bm{\theta}}(\cdot)$. Another (transposed convolutional) neural network is used for the pre-image map $\psi_{\bm{\zeta}}(\cdot)$ \cite{dumoulin2016guide}. The network parameters $\{\bm{\theta},\bm{\zeta}\}$ are learned by minimizing the reconstruction errors defined by $ \mathcal{L}_1(\bm{x}_i^\star,\psi_{1_{\bm{\zeta}_{1}}} (\phi_{1_{\bm{\theta}_{1}}} (\bm{x}_i^\star)))$ and $\mathcal{L}_2(\bm{y}_i^\star,\psi_{2_{\bm{\zeta}_{2}}} (\phi_{2_{\bm{\theta}_{2}}} (\bm{y}_i^\star)))$. In our experiments, we use the mean-squared errors $\mathcal{L}_1(\bm{x}_i^\star,\psi_{1_{\bm{\zeta}_{1}}} (\phi_{1_{\bm{\theta}_{1}}} (\bm{x}_i^\star))) =  \frac{1}{N} \norm{\bm{x}^\star_i - \psi_{1_{\bm{\zeta}_{1}}} (\phi_{1_{\bm{\theta}_{1}}} (\bm{x}_i^\star))}^{2}_2$ and $\mathcal{L}_2(\bm{y}_i^\star,\psi_{2_{\bm{\zeta}_{2}}} (\phi_{2_{\bm{\theta}_{2}}} (\bm{y}_i^\star))) =  \frac{1}{N} \norm{\bm{y}^\star_i - \psi_{2_{\bm{\zeta}_{2}}} (\phi_{2_{\bm{\theta}_{2}}} (\bm{y}_i^\star))}^{2}_2$, however, in principle, one can use any other loss appropriate to the dataset.
Here $\phi_{1_{\bm{\theta}_{1}}} (\bm{x}_i^\star)$ and $\phi_{2_{\bm{\theta}_{2}}} (\bm{y}_i^\star)$ are computed from Eq. \ref{eq:impl_sol}, i.e., the generated points in feature space from the subspace $\mathcal{H}$.

Adding the loss function directly into the objective function $\mathcal{J}_t$ is not suitable for minimization. Instead, we use the stabilized objective function defined as $\mathcal{J}_{stab} = \mathcal{J}_{t} + \frac{c_{\mathrm{stab}}}{2} \mathcal{J}_{t}^2 $, where $c_{stab}\in \mathbb{R}^{+}$ is the regularization constant \cite{suykens_deep_2017}. This tends to push the objective function $\mathcal{J}_t$ towards zero, which is also the case when substituting the solutions $\lambda_i , \bm{h}_i$ back into $\mathcal{J}_t$ (see Appendix \ref{sec:stabilizationTerm} for details). The combined training objective is given by:
%\begin{equation}
%    \label{eq:combined_trainigEq}
%    \mathcal{J}_{c}  = \mathcal{J}_{stab} + \frac{c_{\mathrm{acc}}}{2 N}\left(\sum_{i=1}^N\left[ \norm{\bm{x}^\star_i - \psi_1 (\phi_1 (\bm{x}_i^\star))}^{2}_2  + \norm{\bm{y}^\star_i - \psi_2 (\phi_2 (\bm{y}_i^\star))}^{2}_2 \right] \right), \\
%\end{equation}
\begin{equation}
    \label{eq:combined_trainigEq}
    \underset{\bm{\theta}_1,\bm{\theta}_2,\bm{\zeta}_1,\bm{\zeta}_2}{\min} \mathcal{J}_{c} \enskip  = \mathcal{J}_{stab} + \frac{c_{\mathrm{acc}}}{2 N}\left(\sum_{i=1}^N\left[ \mathcal{L}_1(\bm{x}_i^\star,\psi_{1_{\bm{\zeta}_{1}}} (\phi_{1_{\bm{\theta}_{1}}} (\bm{x}_i^\star)))  + \mathcal{L}_2(\bm{y}_i^\star,\psi_{2_{\bm{\zeta}_{2}}} (\phi_{2_{\bm{\theta}_{2}}} (\bm{y}_i^\star))) \right] \right), \\
\end{equation}
where $c_{\mathrm{acc}}\in \mathbb{R}^+$ is a regularization constant to control the stability with reconstruction accuracy. In this way, we integrate feature-selection and subspace learning within the same training procedure.

\section{The Gen-RKM Algorithm \label{sec:algo}}
%
Based on the previous analysis, we propose a novel algorithm, called the Gen-RKM algorithm, combining kernel learning and generative models. We show that this procedure is efficient to train and evaluate. It is also scalable to large datasets when using explicit feature maps. The training procedure simultaneously involves feature selection, common-subspace learning and inverse-map learning. This is achieved via an optimization procedure where one iteration involves an eigendecomposition of the kernel matrix which is composed of the features from various views (see Eq. \ref{eq:sup_KPCA}). The latent variables are given by the eigenvectors, which are then passed via a pre-image map to reconstruct the sample. The  reconstruction error together with the energy function represents the cost that needs to be minimized. Fig. \ref{fig:schematic_Gen_rkm} shows a schematic representation of the algorithm when two data sources are available.

Thanks to training in  $m$ mini-batches, this procedure is scalable to large datasets (sample size $N$) with training time scaling super-linearly with $ T_m = c \frac{N^{\gamma}}{m^{\gamma -1}}$, instead of $ T_k = c N^{\gamma} $, where $\gamma \approx 3$ for algorithms based on decomposition methods, with some proportionality constant $c$. The training time could be further reduced by computing the covariance matrix (size $(d_{f}+p_{f}) \times (d_{f}+p_{f})$) instead of a kernel matrix (size $\frac{N}{m} \times \frac{N}{m}$), when the sum of the dimensions of the feature-spaces is less than the samples in mini-batch i.e. $d_{f}+p_{f}\leq \frac{N}{m}$. While using neural networks as feature maps, $d_{f}$ and $p_{f}$ correspond to the number of neurons in the output layer, which are chosen as hyperparameters by the practitioner. Eigendecomposition of this smaller covariance matrix would yield $\bm{U}$ and $\bm{V}$ as eigenvectors (see Eq. \ref{eq: cov_mat} and Appendix \ref{subsec:cov_mat} for detailed derivation), where computing the $\bm{h}_i$ involves only matrix-multiplication which is readily parallelizable on modern GPUs:
%
\begin{equation}\label{eq: cov_mat}
    \begin{aligned}
        \begin{bmatrix}
            \frac{1}{\eta_1} \Phi_{\bm{x}}\Phi_{\bm{x}}^{\top} & \frac{1}{\eta_1}\Phi_{\bm{x}}\Phi_{\bm{y}}^{\top} \\
            \frac{1}{\eta_2}\Phi_{\bm{y}}\Phi_{\bm{x}}^{\top}  & \frac{1}{\eta_2}\Phi_{\bm{y}}\Phi_{\bm{y}}^{\top}
        \end{bmatrix}
        \begin{bmatrix}
            \bm{U} \\ \bm{V}
        \end{bmatrix} =
        \begin{bmatrix}
            \bm{U} \\ \bm{V}
        \end{bmatrix}\Lambda, & \quad \begin{aligned}
            \Phi_{\bm{x}} & \coloneqq\left[ \phi_{1}(\bm{x}_{1}),\dots, \phi_{1}(\bm{x}_{N}) \right], \\
            \Phi_{\bm{y}} & \coloneqq\left[ \phi_{2}(\bm{y}_{1}),\dots, \phi_{2}(\bm{y}_{N}) \right].
        \end{aligned}
    \end{aligned}
\end{equation}
%
%
\begin{algorithm}[]
    \caption{Gen-RKM}\label{algo}
    \small
    \textbf{Input:} $ \{\bm{x}_{i}, \bm{y}_{i}\}_{i=1}^N,~\eta_{1},~\eta_{2} $, feature map $ \phi_j ({\cdot}) $ - explicit \emph{or} implicit via kernels $ k_{j}(\cdot, \cdot), \text{for } j \in \{1, 2\} $ \\
    \textbf{Output:} Generated data $ \bm{x}^{\star},~\bm{y}^{\star}  $
    \vspace*{-10pt}
    \begin{multicols}{2}
        \begin{algorithmic}[1]
            \Procedure{Train}{}
            \If{$\phi_j ({\cdot})$ = Implicit}
            \State Hyperparameters: kernel specific
            \State Solve Eq. \ref{eq:sup_KPCA}
            \State Select $s$ principal components
            \ElsIf{$\phi_j ({\cdot})$ = Explicit}
            \While{not converged}
            \State $\{\bm{x}, \bm{y}\} \gets \text{\{Get mini-batch\}} $
            \State $ \phi_{1}(\bm{x})\gets \bm{x}; ~\phi_{2}(\bm{y})\gets \bm{y}  $
            \State do steps $ 4\text{-}5 $
            \State $\{\phi_{1}(\bm{x}), \phi_{2}(\bm{y})\} \gets h $ (Eq. \ref{eq:impl_sol})
            \State $ \{\bm{x}, \bm{y}\} \gets \{\psi_1(\phi_{1}(\bm{x})), \psi_2(\phi_{2}(\bm{y}))\} $
            \State $ \Delta \bm{\theta}_1 \propto -\nabla_{\bm{\theta}_1} \mathcal{J}_{c} $; $~ \Delta \bm{\theta}_2 \propto -\nabla_{\bm{\theta}_2} \mathcal{J}_{c} $
            \State $ \Delta \bm{\zeta}_1 \propto -\nabla_{\bm{\zeta}_1} \mathcal{J}_{c} $; $~ \Delta \bm{\zeta}_2 \propto -\nabla_{\bm{\zeta}_2} \mathcal{J}_{c} $
            \EndWhile
            \EndIf
            \EndProcedure
        \end{algorithmic}

        \columnbreak

        \begin{algorithmic}[1]
            \Procedure{Generation}{}
            \State Select $ \bm{h}^{\star}$
            \If{$\phi_j ({\cdot})$ = Implicit}
            \State Hyperparameter: $ n_{r} $
            \State Compute $ \bm{k}_{\bm{x}^{*}},~\bm{k}_{\bm{y}^{*}} $ (Eq. \ref{eq:Kxy})
            \State $ \text{Get }{\hat{\bm{x}}},~{\hat{\bm{y}}} $ (Eq. \ref{eq: pre-Ix,y})
            \ElsIf{$\phi_j ({\cdot})$ = Explicit}
            \State do steps $ 11\text{-}12$
            \EndIf

            \EndProcedure
        \end{algorithmic}
    \end{multicols}
\end{algorithm}
%
\begin{figure}
	\centering
	\begin{subfigure}[b]{0.40\textwidth}
    	\includestandalone[]{fig2a}
        \caption{MNIST}
        \label{fig:mnist}
    \end{subfigure}
    % %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.40\textwidth}
        \includestandalone[]{fig2b}
        \caption{Fashion-MNIST}
        \label{fig:fashion}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.40\textwidth}
        \includestandalone[]{fig2c}
        \caption{CIFAR-10}
        \label{fig:cifar-10}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.40\textwidth}
        \includestandalone[]{fig2d}
        \caption{CelebA}
        \label{fig:celeba}
    \end{subfigure}
    \caption{Generated samples from the model using CNN as explicit feature map in the kernel function. The yellow boxes in the first column show training examples and the adjacent boxes show the reconstructed samples. The other images (columns 3-6) are generated by random sampling from the fitted distribution over the learned latent variables.}\label{fig:gen_cov}
\end{figure}
%
%
\begin{figure}
	\centering
	\includestandalone[]{fig3celeb}
    % \end{center}
    \caption{Multi-view generation on CelebA dataset showing images and attributes.\label{fig:celeb_mul_lab}}
\end{figure}
%
%
\begin{figure}
	\centering
	\begin{subfigure}[b]{\textwidth}
		\centering
		\includestandalone[]{fig4a}
		\caption{MNIST: Implicit feature maps with Gaussian kernel are used during training. For generation, the pre-images are computed using the kernel-smoother method.}
		\label{fig:kernel_smooth_mnist}
	\end{subfigure}
%	\\
	\begin{subfigure}[b]{\textwidth}
		\centering
		\includestandalone[]{fig4b}
		\caption{MNIST: Explicit feature maps and the corresponding pre-image maps are defined by the Convolutional Neural Networks.}
		\label{fig:mul_view_b}
	\end{subfigure}
%	\\
	\begin{subfigure}[b]{\textwidth}
		\centering
		\includestandalone[]{fig4c}
		\caption{CIFAR-10: Explicit feature maps as Convolutional Neural Networks. Pre-images are computed using Transposed CNNs.}
		\label{fig:mul_view_c}
	\end{subfigure}
	%
	\caption{Multi-view Generation (images and labels) on various datasets using implicit and explicit feature maps.}
	\label{fig:multi-view_gen}
\end{figure}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments \label{sec:exp}}
%
To demonstrate the applicability of the proposed framework and algorithm, we trained the Gen-RKM model on a variety of datasets commonly used to evaluate generative models: MNIST \cite{lecun-mnisthandwrittendigit-2010}, Fashion-MNIST \cite{xiao2017/online}, CIFAR-10 \cite{cifar_10}, CelebA \cite{liu2015faceattributes} and Dsprites \cite{dsprites17}.
%
The experiments were performed using both the implicit feature map defined by a Gaussian kernel and parametric explicit feature maps defined by deep neural networks, either Convolutional or fully connected. As explained in Section \ref{sec:GRKM}, in case of kernel methods, training only involves constructing the kernel matrix and solving the eigenvalue problem in Eq. \ref{eq:sup_KPCA}. In principle, one could also use the latent variables directly for generation. However, in our experiments, we fit a Gaussian mixture model (GMM) with $l$ components to the latent variables of the training set, and randomly sample a new point $\bm{h}^{\star}$ for generating views using a kernel smoother. In case of explicit feature maps, we define $\phi_{1_{\bm{\theta}_1}}$ and $\psi_{1_{\bm{\zeta}_1}}$ as convolution and transposed-convolution neural networks, respectively \cite{dumoulin2016guide}; and $\phi_{2_{\bm{\theta}_2}}$ and $\psi_{1_{\bm{\zeta}_2}}$ as fully-connected networks. The particular architecture details are outlined in Table \ref{tab:arch} in the Appendix. The training procedure in case of explicitly defined maps consists of minimizing $\mathcal{J}_{c}$ using the Adam optimizer \cite{Adam} to update the weights and biases. To speed-up learning, we subdivided the datasets into $m$ mini-batches, and within each iteration of the optimizer, Eq. \ref{eq:sup_KPCA} is solved to update the value of $\bm{H}$. Information on the datasets and hyperparameters used for the experiments is given in Table \ref{Table:dataset} in the Appendix.
%
%

\noindent \textbf{Generation:}
Figure \ref{fig:mnist} shows the generated images using a kernel smoother method. The first column in yellow-boxes shows the training samples and the second column on the right shows the reconstructed samples. The other images shown are generated by random sampling from a GMM over the learned latent variables. Notice that the reconstructed samples are of better quality visually than the other images generated by random sampling. Figures \ref{fig:fashion}, \ref{fig:cifar-10} and \ref{fig:celeba} show the images generated when the convolutional neural network  and transposed-convolutional neural network was used as the feature map and pre-image map respectively. To elucidate that the model has not merely memorized the training examples, we show the generated images via bilinear-interpolations of the latent variables in Appendix \ref{appen:further_results}.
%
%

\noindent \textbf{Multi-view Generation:}
%
%NOT FINAL \\
Figures \ref{fig:celeb_mul_lab} \& \ref{fig:multi-view_gen} demonstrate the multi-view generative capabilities of the model. In these datasets, labels or attributes are seen as another view of the image that provides extra information. One-hot encoding of the labels was used to train the model. Figure \ref{fig:kernel_smooth_mnist} shows the generated images and labels when feature maps are only implicitly known i.e. through a Gaussian kernel. Figures \ref{fig:mul_view_b}, \ref{fig:mul_view_c} shows the same when using fully-connected networks as parametric functions to encode and decode labels. We can see that both the generated image and the generated label matches in most cases, albeit not all. Up to our knowledge, no universal evaluation metric exists to assess such characteristic of multi-view generation. Though one can use classifiers to crudely assess the matching, however, depending on the type of classifier and the way it was trained, the results would vary among researchers.

%
\begin{figure}[h]
	\centering
	\includestandalone[width=0.85\textwidth]{fig5_unco}
	\caption{Exploring the learned uncorrelated-features by traversing along the eigenvectors. The first column shows the scatter plot of latent variables using the top two principal components. The green lines within, show the traversal in the latent space and the related rows show the corresponding reconstructed images.}
	\label{fig:uncorre}
\end{figure}
%
%
\noindent \textbf{Targeted Generation:}
Since the components of the latent variables are the eigenvectors of the kernel matrix (see Eq. \ref{eq:sup_KPCA}), one can exploit the orthogonality for targeted generation. Such targeted generation capabilities could be useful in critical applications where the data needs to be generated based on some prior-knowledge or with specific attributes.
%
We explore the uncorrelated features learned by the models on the Dsprites and celebA dataset (See Fig. \ref{fig:uncorre}). In our experiments, the Dsprites training dataset comprised of $32\times 32$ positions of oval and heart-shaped objects. The number of principal components chosen were 2 and the goal was to find-out whether traversing along the eigenvectors, corresponds to traversing the generated image in one particular direction while preserving the shape of the object. Rows 1 and 2 of Fig. \ref{fig:uncorre} show the reconstructed images of an oval while moving along first and second principal component respectively. Notice that the first and second components correspond to the $y$ and $x$ positions respectively. Rows 3 and 4 show the same for hearts. On the celebA dataset, we train the Gen-RKM with 15 components. Rows 5 and 6 shows the reconstructed images while traversing along the principal components. When moving along the first component from left-to-right, the hair-color of the women transforms, while preserving the face structure. Whereas traversal along the second component, transforms a man to woman while preserving the orientation. When the number of principal components were 2 while training, the brightness and  background light-source corresponds to the two largest variances in the dataset. Also notice that, the reconstructed images are more blurry due to the selection of less number of components to model $\mathcal{H}$.
%
%
% However in principle, one can use any pre-image method.
% 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and future work \label{sec:conc}}
%
% Talk about the fact that NNs are a powerful feature-maps which enabled the classical KPCA to learn such powerful representations, it is worth exploring about how the features are learned (through recon error) and then kernel decomposition extracts the most distinct variations in those features.
%
%SHORTEN\\
The paper proposes a novel framework, called Gen-RKM, for generative models based on RKMs with extensions to multi-view generation and learning uncorrelated representations. This allows for a  mechanism where the feature map can be defined using kernel functions or (deep) neural network based methods. When using kernel functions, the training consists of only solving an eigenvalue problem. In the case of a (convolutional) neural network based explicit feature map, we used (transposed) networks as the pre-image functions. Consequently, a training procedure was proposed which involves joint feature-selection and subspace learning. Thanks to training in mini-batches and capability of working with covariance matrices, the training is scalable to large datasets. Experiments on benchmark datasets illustrate the merit of the proposed framework. Furthermore, a targeted generation mechanism is demonstrated which uses the uncorrelated features modelled by the orthogonal eigenvectors. Extensions of this work consists of adapting the model to more advanced multi-view datatsets involving speech, images and texts; further analysis on other feature maps, pre-image methods, loss-functions and uncorrelated feature learning. Finally, this paper has demonstrated the applicability of the Gen-RKM framework, suggesting new research directions to be worth exploring.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsubsection*{Acknowledgments}
% The research leading to these results has received funding from the European Research Council under the European Union's Horizon $2020$ research and innovation program / ERC Advanced Grant E-DUALITY (787960). This paper reflects only the authors' views and the Union is not liable for any use that may be made of the contained information. The research is further supported by Research Council KUL: Optimization frameworks for deep kernel machines $C14/18/068$, Flemish Government: FWO project $GOA4917N$ (Deep Restricted Kernel Machines: Methods and Foundations), PhD/Postdoc grant; Impulsfonds AI: $VR 2019 2203 DOC.0318/1QUATER$ Kenniscentrum Data en Maatschappij and by Ford KU Leuven Research Alliance Project $KUL0076$ (Stability analysis and performance improvement of deep reinforcement learning algorithms).
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\bibliography{references}
\bibliographystyle{iclr2020_conference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\section{Appendix}
%
\subsection{Derivation of Gen-RKM objective function}
\label{subsec: Gen-RKM objective function}
Given $\mathcal{D}= \{\bm{x}_i, \bm{y}_i\}_{i=1}^{N} $, where $ \bm{x}_i \in \mathbb{R}^d $, $ \bm{y}_i \in \mathbb{R}^p $ and feature-map $\phi_{1}: \mathbb{R}^{d}\mapsto \mathbb{R}^{d_{f}}$ and $\phi_{2}: \mathbb{R}^{p}\mapsto \mathbb{R}^{p_{f}}$, the Least-Squares Support Vector Machine (LS-SVM) formulation of Kernel PCA \cite{suykens_least_2002} for the two data sources can be written as:
\begin{equation}
    \begin{aligned}
        \underset{\bm{U,V,e}_i}{\min} & ~\dfrac{\eta_{1}}{2}\tr(\bm{U}^\top \bm{U} ) + \dfrac{\eta_{2}}{2}\tr(\bm{V}^\top \bm{V}) - \dfrac{1}{2\lambda}\sum_{i=1}^{N}\bm{e}_{i}^\top \bm{e}_i \\
        \text{s.t.}                   & ~ \bm{e}_i=\bm{U}^{\top}\phi_{1}(\bm{x}_i) + \bm{V}^{\top}\phi_{2}(\bm{y}_i) \quad \forall i=1,\ldots,N,
    \end{aligned}
    \label{eq: lssvm_kpca}
\end{equation}
where $ \bm{U} \in \mathbb{R}^{d \times s} $ and $ \bm{V} \in \mathbb{R}^{p \times s} $ are the interconnection matrices.\\

Using the notion of \emph{conjugate feature duality} introduced in \cite{suykens_deep_2017}, the error variables $\bm{e}_{i}$ are conjugated to latent variables $\bm{h}_{i}$ using:
%
\begin{equation}
    \frac{1}{2\lambda}\bm{e}^{\top}\bm{e} + \frac{\lambda}{2}\bm{h}^{\top}\bm{h} \geq \bm{e}^{\top}\bm{h}, \qquad \forall \bm{e}, \bm{h} \in \mathbb{R}^{s}
    \label{eq: conj_feat}
\end{equation}
which is also known as the Fenchel-Young inequality for the case of quadratic functions \cite{rockafeller1987}. By eliminating the variables $\bm{e}_{i}$ from Eq. \ref{eq: lssvm_kpca} and using Eq. \ref{eq: conj_feat}, we obtain the Gen-RKM training objective function:
%
\begin{equation}
    \mathcal{J}_{t} = \sum_{i=1}^{N} \left(-\phi_{1}(\bm{x}_{i})^\top \bm{U}\bm{h}_i - \phi_{2}(\bm{y}_{i})^\top \bm{V} \bm{h}_i + \frac{\lambda}{2} \bm{h}_{i}^\top \bm{h}_i\right) + \frac{\eta_{1}}{2}\tr(\bm{U}^\top \bm{U}) + \frac{\eta_{2}}{2}\tr(\bm{V}^\top \bm{V}) .
\end{equation}
%
\subsection{Kernel PCA in the primal}
\label{subsec:cov_mat}
%
From Eq. \ref{eq:2}, eliminating the variables $\bm{h}_{i}$ yields the following:
\begin{equation}
    \begin{aligned}
        \frac{1}{\eta_1}\left[ \sum_{i=1}^{N}\phi_{1}(\bm{x}_{i})\phi_{1}(\bm{x}_{i})^\top \bm{U} + \sum_{i=1}^{N}\phi_{1}(\bm{x}_{i})\phi_{2}(\bm{y}_{i})^\top \bm{V}  \right] & = \lambda\bm{U}, \\
        \frac{1}{\eta_2}\left[  \sum_{i=1}^{N}\phi_{2}(\bm{y}_{i})\phi_{1}(\bm{x}_{i})^\top \bm{U} + \sum_{i=1}^{N}\phi_{2}(\bm{y}_{i})\phi_{2}(\bm{y}_{i})^\top \bm{V} \right] & = \lambda\bm{V}.
    \end{aligned}
\end{equation}
%
Denote $\Phi_{\bm{x}}\coloneqq\left[ \phi_{1}(\bm{x}_{1}),\dots, \phi_{1}(\bm{x}_{N}) \right]$, $ \Phi_{\bm{y}}\coloneqq\left[ \phi_{2}(\bm{y}_{1}),\dots, \phi_{2}(\bm{y}_{N}) \right]$ and $ \bm{\Lambda} =\diag\{\lambda_1,\ldots,\lambda_s\}\in\mathbb{R}^{s\times s} $ with $s \leq N$. Now, composing the above equations in matrix form, we get the following eigen-decomposition problem:
%
\begin{equation}\label{eq: cov_mat_appen}
    \begin{bmatrix}
        \frac{1}{\eta_1} \Phi_{\bm{x}}\Phi_{\bm{x}}^{\top} & \frac{1}{\eta_1}\Phi_{\bm{x}}\Phi_{\bm{y}}^{\top} \\
        \frac{1}{\eta_2}\Phi_{\bm{y}}\Phi_{\bm{x}}^{\top}  & \frac{1}{\eta_2}\Phi_{\bm{y}}\Phi_{\bm{y}}^{\top}
    \end{bmatrix}
    \begin{bmatrix}
        \bm{U} \\ \bm{V}
    \end{bmatrix} =
    \begin{bmatrix}
        \bm{U} \\ \bm{V}
    \end{bmatrix}\Lambda.
\end{equation}
%
Here the size of the covariance matrix is $(d_f + p_f)\times (d_f + p_f)$. The latent variables $\bm{h}_{i}$ can be computed using Eq. \ref{eq:2}, which simply involves matrix multiplications.
%
\subsection{Stabilizing the objective function \label{sec:stabilizationTerm}}
%
\begin{proposition}\label{prop2}
    All stationary solutions for $\bm{H}$,$\bm{\Lambda}$ in Eq. \ref{eq:sup_KPCA} of $ \mathcal{J}_{t} $ lead to $ \mathcal{J}_{t}=0 $.
\end{proposition}
\begin{proof}
    Let $\lambda_i, \bm{h}_i$ are given by Eq. \ref{eq:sup_KPCA}. Using Eq. \ref{eq:2} to substitute $ \bm{V} $ and $ \bm{U} $  in Eq. \ref{eq:obj_train} yields:
    \[\begin{aligned}
            \mathcal{J}_t(\bm{V},\bm{U},\bm{\Lambda},\bm{H}) & = \sum_{i=1}^{N} - \frac{\lambda}{2} \bm{h}_{i}^\top \bm{h}_i + \frac{\eta_{1}}{2}\tr\left(\dfrac{1}{\eta_{1}^2} \sum_{i=1}^{N} \bm{h_{i}} \phi_{1}(\bm{x}_{i})^\top  \sum_{j=1}^{N} \phi_{1}(\bm{x}_{j}) \bm{h_{j}}^\top \right) \\
                                                             & \enskip + \frac{\eta_{2}}{2}\tr\left(\dfrac{1}{\eta_{2}^2} \sum_{i=1}^{N} \bm{h_{i}} \phi_{2}(\bm{y}_{i})^\top  \sum_{j=1}^{N} \phi_{2}(\bm{y}_{j}) \bm{h_{j}}^\top \right)
        \end{aligned}
    \]
    %
    \[\begin{aligned}
            = & \sum_{i=1}^{N} - \frac{\lambda}{2} \bm{h}_{i}^\top \bm{h}_i + \frac{\eta_{1}}{2}\tr\left(\dfrac{1}{\eta_{1}^2}\bm{H}\bm{K}_{1}\bm{H}^\top \right) + \frac{\eta_{2}}{2}\tr\left(\dfrac{1}{\eta_{2}^2}\bm{H}\bm{K}_{2}\bm{H}^\top\right) \\
            = & \sum_{i=1}^{N} - \frac{\lambda}{2} \bm{h}_{i}^\top \bm{h}_i + \frac{1}{2}\tr\left(\bm{H} \left[\dfrac{1}{\eta_{1}}\bm{K}_{1} + \dfrac{1}{\eta_{2}}\bm{K}_{2}\right]\bm{H}^\top\right).
        \end{aligned}
    \]
    From Eq. \ref{eq:sup_KPCA}, we get:
    \[\mathcal{J}_t(\bm{V},\bm{U},\bm{\Lambda},\bm{H}) = \sum_{i=1}^{N} - \frac{\lambda}{2} \bm{h}_{i}^\top \bm{h}_i + \frac{1}{2}\tr\left(\bm{H} \bm{H}^\top \lambda \right)
        = \sum_{i=1}^{N} - \frac{\lambda}{2} \bm{h}_{i}^\top \bm{h}_i + \frac{\lambda}{2}\sum_{i=1}^{N} \bm{h}_{i}^\top \bm{h}_{i}
        =   0.
    \]
\end{proof}
%
%
\begin{proposition}\label{prop1}
    Let $ J(\bm{x}):\mathbb{R}^N\xrightarrow{}\mathbb{R} $ be a smooth function, for all $ \bm{x}\in \mathbb{R}^N$ and for $ c\in \mathbb{R}_{>0} $,   define $ \bar{J}(\bm{x}) := J(\bm{x}) + \dfrac{c}{2}J(\bm{x})^{2} $. Assuming $(1+cJ(\bm{x}))\neq 0$, then $\bm{x}^{\star}$ is the stationary points of $ \bar{J}(\bm{x})$ iff $\bm{x}^{\star}$ is the stationary point for $ {J}(\bm{x})$.
\end{proposition}
\begin{proof}
    Let $ \bm{x}^{\star} $ be a stationary point of $ J(\bm{x}) $, meaning that $ \nabla J(\bm{x}^{\star}) = 0$. The stationary points for $ \bar{J}(\bm{x})$ can be obtained from:
    \begin{equation}\label{eq: stat_pt_jx}
        \dfrac{d \bar{J}}{d \bm{x}} =\left(\nabla J(\bm{x}) + cJ(\bm{x})\nabla J(\bm{x})\right)
        = \left(1+c J(\bm{x})\right)\nabla J(\bm{x}).
    \end{equation}
    It is easy to see from Eq. \ref{prop1} that if $ \bm{x}=\bm{x}^{*} $, $ \nabla J(\bm{x}^*) = 0$, we have that $ \dfrac{d \bar{J}}{d \bm{x}}\Big|_{\bm{x}^*}=0$, meaning that all the stationary points of  $ J(\bm{x}) $ are stationary points of $ \bar{J}(\bm{x}) $.

    To show the other way, let $\bm{x}^{\star}$ be stationary point of $\bar{J}(\bm{x})$ i.e. $\nabla \bar{J}(\bm{x}^{\star})=0.$ Assuming $(1+cJ(\bm{x}^{\star}))\neq 0$, then from Eq. \ref{eq: stat_pt_jx} for all $ c\in \mathbb{R}_{>0}$, we have
    %
    \begin{equation*}
        \left(1+cJ(\bm{x}^{\star})\right)\nabla J(\bm{x}^{\star})=0,
    \end{equation*}
    %
    implying that $ \nabla J(\bm{x}^{\star}) = 0$.
    %
    %Now if $\bm{x}^*$ is also a minimizer for $J(\bm{x})$ i.e. $ \Delta J(\bm{x}^*)> 0$,  then the minimizers of $\bar{J}(\bm{x})$ at $\bm{x}^*$  takes the form:
    %$$\frac{d^2  \bar{J}}{d \bm{x}^2}\Big|_{\bm{x}^*} = c(\nabla J(\bm{x}^*))^2 + (1+c J(\bm{x}^*))\Delta J(\bm{x}^*). $$
    %Then, $\bm{x}^{\star}$ is also minimizer for  $ \bar{J}(\bm{x})$ i.e. $\frac{d^2  \bar{J}}{d \bm{x}^2}\Big|_{\bm{x}^*} > 0,$ if  $\left(1+c J(\bm{x}^*)\right)\geq 0 $, implying $J(\bm{x}^*)\geq-\frac{1}{c}$.
    %
    %The lower bound for the minimizer of $\bar{J}(\bm{x})$ can be estimated by substituting $J(\bm{x}^*)\geq-\frac{1}{c}$ into definition leading to $\bar{J}(\bm{x}^*)\geq-\frac{1}{2c}$. Also, a much weaker bound for $\bar{J}(\bm{x}^*)$ can be extracted from the definition of $\bar{J}(\bm{x})$ as
    %$\bar{J}(\bm{x}^*)\geq {J}(\bm{x}^*)\geq -\frac{1}{c}$.
\end{proof}
%
Based on the above propositions, we stabilize our original objective function Eq. \ref{eq:obj_train} to keep it bounded and hence is suitable for minimization with Gradient-descent methods. Without the reconstruction errors, the stabilized objective function is
\[
    \underset{\bm{U},\bm{V},\bm{h_{i}}}{\min}\mathcal{J}_{t} + \frac{c}{2}\mathcal{J}_{t}^2.
\]
%
Denoting $ \bar{J} = \mathcal{J}_{t} + \frac{c_{stab}}{2}\mathcal{J}_{t}^2 $. Since the derivatives of $ \mathcal{J}_{t} $ are given by Eq. \ref{eq:2}, the stationary points of $ \bar{J} $ are:
\[
    \begin{cases}
        \frac{\partial \bar{J}}{\partial \bm{V}} = \left(1 + c_{stab}\mathcal{J}_{t} \right)\left( - \sum_{i=1}^{N}\phi_{1}(\bm{x}_{i})\bm{h}_{i}^\top + \eta_{1}\bm{V}\right) = 0                       & \implies {\bm{V} = \frac{1}{\eta_{1}} \sum_{i=1}^{N}\phi_{1}(\bm{x}_{i})\bm{h}_{i}^\top }, \\
        \frac{\partial \bar{J}}{\partial \bm{U}} = \left(1 + c_{stab}\mathcal{J}_{t} \right)\left( - \sum_{i=1}^{N}\phi_{2}(\bm{y}_{i})\bm{h}_{i}^\top + \eta_{2}\bm{U}\right) = 0                       & \implies {\bm{U} = \frac{1}{\eta_{2}} \sum_{i=1}^{N}\phi_{2}(\bm{y}_{i})\bm{h}_{i}^\top }, \\
        \frac{\partial \bar{J}}{\partial \bm{h}_{i}} = \left(1 + c_{stab}\mathcal{J}_{t} \right)\left( -  \bm{V}^\top \phi_{1}(\bm{x}_i) - \bm{U}^\top \phi_{2}(\bm{y}_i) + \lambda \bm{h}_i \right) = 0 & \implies \begin{aligned}\lambda \bm{h}_i & =  \bm{V}^\top \phi_{1}(\bm{x}_i) \\ &+ \bm{U}^\top \phi_{2}(\bm{y}_i),
        \end{aligned}
    \end{cases}
\]
assuming $ 1 + c_{stab}\mathcal{J}_{t} \neq 0 $. Elimination of $ \bm{V} ~ \text{and} ~ \bm{U} $ yields $     {\left[ \frac{1}{\eta_{1}}\bm{K}_{1} + \frac{1}{\eta_{2}}\bm{K}_{2} \right] \bm{H}^\top=\bm{H}^\top \bm{\Lambda}} $, which is indeed the same solution for $ c_{stab}=0 $ in Eq.~\ref{eq:obj_train} and Eq.~\ref{eq:sup_KPCA}.
%
\subsection{Centering of kernel matrix \label{sec:centering}}
%
Centering of the kernel matrix is done by the following equation:
\begin{equation}\label{eq:centering}
    \bm{K}_{c}= \bm{K} - N^{-1}\bm{11}^{\top} \bm{K} - N^{-1}\bm{K} \bm{11}^{\top} + N^{-2}\bm{11}^{\top}\bm{K}\bm{11}^{\top},
\end{equation}
where $\bm 1$ denotes an $N$-dimensional vector of ones and $ \bm{K} $ is either $ \bm{K}_{1} $ or $ \bm{K}_{2} $.
%
%
\subsection{Architecture details}
\label{subsec: Architecture}
%
See Table \ref{Table:dataset} and \ref{tab:arch} for details on model architectures, datasets and hyperparameters used in this paper. The PyTorch library in Python was used as the programming language with a 8GB NVIDIA QUADRO P4000 GPU.
%
\begin{table}[H]
    \caption{Datasets and hyperparameters used for the experiments.}
    \label{Table:dataset}
    \centering
    \begin{tabular}{lccccccccc}
        \toprule
        Dataset       & $N$    & $d$                       & $N_{\mathrm{subset}}$ & $s$ & $m$ & $\sigma$ & $n_r$ & $l$ \\ \midrule
        MNIST         & 60000  & $28 \times 28$            & 5000                  & 500 & 50  & 1.3      & 4     & 10  \\
        Fashion-MNIST & 60000  & $28 \times 28$            & 500                   & 100 & 5   & /        & /     & 10  \\
        CIFAR-10      & 60000  & $32 \times 32 \times 3$   & 500                   & 500 & 5   & /        & /     & 10  \\
        CelebA        & 202599 & $128 \times 128 \times 3$ & 500                   & 15  & 5   & /        & /     & 20  \\
        Dsprites      & 737280 & $64 \times 64$            & 1024                  & 2   & 5   & /        & /     & /   \\ \bottomrule
    \end{tabular}
\end{table}
%
%
\begin{table}[H]    \caption{Details of model architectures used in the paper. All convolutions and transposed-convolutions are with stride 2 and padding 1. Unless stated otherwise, the layers have Parametric-RELU ($\alpha = 0.2$) activation function, except the output layers of the pre-image maps which has sigmoid activation function. \label{tab:arch}}
    \begin{tabular}{l l l l l}
        \specialrule{.1em}{.05em}{.05em}
        \textbf{Dataset}          & \textbf{Optimizer}         &                    & \multicolumn{2}{c}{\textbf{Architecture}}                                       \\
                                  & \multicolumn{1}{c}{(Adam)} &                    & \multicolumn{1}{c}{$ \mathcal{X} $}       & \multicolumn{1}{c}{$ \mathcal{Y} $} \\
        \specialrule{.1em}{.05em}{.05em}
        \multirow{4}{*}{MNIST}    & \multirow{4}{*}{1e-3}      & Input              & 28x28x1                                   & 10 (One-hot encoding)               \\
                                  &                            & Feature-map (fm)   & \pbox{20cm}{Conv 32x4x4;                                                        \\  Conv 64x4x4;\\ FC 128 (Linear)} &      FC 15, 20 (Linear)                 \\
                                  &                            & Pre-image map      & reverse of fm                             & reverse of fm                       \\
                                  &                            & Latent space dim.  & \multicolumn{2}{c}{$500$}                                                       \\
        \hline
        \multirow{4}{*}{\pbox{20cm}{Fashion                                                                                                                           \\-MNIST}} & \multirow{4}{*}{1e-3} & Input                  & 28x28x1                                   & 10 (One-hot encoding)      \\
                                  &                            & Feature-map        & \pbox{20cm}{Conv 32x4x4;                                                        \\  64x4x4;\\ FC 128 (Linear)}                              &           FC 15, 20                                \\
                                  &                            & Pre-image map (fm) & reverse of fm                             & reverse of fm                       \\
                                  &                            & Latent space dim.  & \multicolumn{2}{c}{$100$}                                                       \\
        \hline
        %
        \multirow{4}{*}{CIFAR-10} & \multirow{4}{*}{1e-3}      & Input              & 32x32x3                                   & 10 (One-hot encoding)               \\
                                  &                            & Feature-map (fm)   & \pbox{20cm}{Conv 64x4x4;                                                        \\  Conv 128x4x4;\\ FC 128 (Linear)}                                   &             FC 15, 20                \\
                                  &                            & Pre-image map      & reverse of fm                             & reverse of fm                       \\
                                  &                            & Latent space dim.  & \multicolumn{2}{c}{$500$}                                                       \\
        \hline
        %
        \multirow{4}{*}{CelebA}   & \multirow{4}{*}{1e-4}      & Input              & 64x64x3                                   & -                                   \\
                                  &                            & Feature-map (fm)   & \pbox{20cm}{Conv 32x4x4;                                                        \\ Conv 64x4x4;\\ Conv 128x4x4;\\Conv 256x4x4 ;\\ FC 128 (Linear)}                                          &            -                \\
                                  &                            & Pre-image map      & reverse of fm                             & -                                   \\
                                  &                            & Latent space dim.  & \multicolumn{2}{c}{$15$}                                                        \\
        \hline
        %
        \multirow{4}{*}{Dsprites} & \multirow{4}{*}{1e-4}      & Input              & 64x64x1                                   & -                                   \\
                                  &                            & Feature-map (fm)   & \pbox{20cm}{Conv 20x4x4;                                                        \\ Conv 40x4x4;\\ Conv 80x4x4;\\ FC 300 (Linear)}                                        &           -            \\
                                  &                            & Pre-image map      & reverse of fm                             & -                                   \\
                                  &                            & Latent space dim.  & \multicolumn{2}{c}{$2$}                                                         \\
        \hline
        %
    \end{tabular}
\end{table}
%
%
\subsection{Further empirical results}\label{appen:further_results}
%
In the following section, we show generated images by bilinear-interpolation of latent vectors. Given four vectors $\bm{h}_1 , \bm{h}_2 , \bm{h}_3 \text{ and } \bm{h}_4 $ (reconstructed images from these vectors are shown at the edges of Figs. \ref{fig:mnist_2d}, \ref{fig:celeb_2d}),  the interpolated vector $\bm{h}^{\star}$ is given by:
\[
    \bm{h}^{\star} = (1-\alpha)(1-\gamma)\bm{h}_1 + \alpha (1-\gamma)\bm{h}_2 + \gamma (1-\alpha)\bm{h}_3 + \gamma \alpha\bm{h}_4, \quad 0\leq \alpha, \gamma \leq 1.
\]
This $\bm{h}^{\star}$ is then used in step 8 of the generation procedure of Gen-RKM algorithm (see Algorithm \ref{algo}) to compute $\bm{x}^{\star}$.
%
\begin{figure}[h]
    \centering
 	\includestandalone[width=0.8\textwidth]{fig_2d_mnist}
    %
    \caption{Reconstructed images by bilinear-interpolation in latent space. The model was trained using the Binary Cross-entropy reconstruction loss.}
    \label{fig:mnist_2d}
\end{figure}
%
%
\begin{figure}[h]
    \centering
 	\includestandalone[width=0.8\textwidth]{fig_2d_celeb}
    \caption{CelebA: Reconstructed images by bilinear-interpolation in latent space.}
    \label{fig:celeb_2d}
\end{figure}
%
%
%
\end{documen
t}
\end{document}
